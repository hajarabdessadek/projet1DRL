{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1b2c7-0146-44cb-a9b6-c70a6de8367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercice1\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('FrozenLake-v1',is_slippery=True,render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render() \n",
    "print (f\"l'espace d'actions: {env.action_space}\")\n",
    "print (f\"l'espace d'observatios: {env.observation_space}\")\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    obs, reward, done, _,_ = env.step(action)  \n",
    "    print(f\"State: {obs}, Reward: {reward}, Done: {done},action:{action}\")\n",
    "\n",
    "    if done:\n",
    "        env.reset()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c3be2-3ae3-419f-a10f-2d4b9fa282c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercice2\n",
    "import numpy as np\n",
    "n_states = env.observation_space.n \n",
    "n_actions = env.action_space.n \n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "print (\"Q_table initialisée:\")\n",
    "print(Q_table)\n",
    "env.close()\n",
    "for state in range(n_states):\n",
    "    print(f\"État {state}: {Q_table[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59a845-df13-46db-93ba-690cc813b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercice3\n",
    "alpha=0.1\n",
    "gamma=0.99\n",
    "epsilon=1.0\n",
    "epsilon_decay=0.995\n",
    "num_episodes=5000\n",
    "\n",
    " \n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "       \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() \n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])  \n",
    "        \n",
    "       \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action])\n",
    "        \n",
    "        # Passer à l'état suivant\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    # Afficher la récompense totale après chaque épisode\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Épisode {episode + 1}, Récompense totale : {total_rewards}\")\n",
    "\n",
    "# Afficher la Q-table finale\n",
    "print(\"\\nQ-table finale après l'entraînement :\")\n",
    "print(Q_table)\n",
    "\n",
    "env.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869072cd-45fd-4838-ab29-485e7f95b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Créer l'environnement sans mode 'human' pour éviter l'erreur de rendu\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)\n",
    "\n",
    "# Initialisation de la Q-table\n",
    "Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Paramètres\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 5000\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  \n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Exploration : action aléatoire\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])  # Exploitation : action avec la meilleure Q-value\n",
    "        \n",
    "        # Appliquer l'action à l'environnement\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # Mise à jour de la Q-table\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action])\n",
    "        \n",
    "        # Passer à l'état suivant\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    # Afficher la récompense totale après chaque épisode\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Épisode {episode + 1}, Récompense totale : {total_rewards}\")\n",
    "\n",
    "    # Mise à jour de epsilon (réduction progressive de l'exploration)\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01) \n",
    "\n",
    "# Afficher la Q-table finale\n",
    "print(\"\\nQ-table finale après l'entraînement :\")\n",
    "print(Q_table)\n",
    "\n",
    "env.close()  # Fermer l'environnement après l'entraînement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3d33a-749e-4f1a-9c49-153688940970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercice1\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True,render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render() \n",
    "print (f\"l'espace d'actions: {env.action_space}\")\n",
    "print (f\"l'espace d'observatios: {env.observation_space}\")\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    obs, reward, done, _,_ = env.step(action)  \n",
    "    print(f\"State: {obs}, Reward: {reward}, Done: {done},action:{action}\")\n",
    "\n",
    "    if done:\n",
    "        env.reset()  \n",
    "#exercice2\n",
    "Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print (\"Q_table initialisée:\")\n",
    "print(Q_table)\n",
    "\n",
    "for state in range(env.observation_space.n):\n",
    "    print(f\"État {state}: {Q_table[state]}\")\n",
    " #exercice3   \n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 5000\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset() \n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    while not done:\n",
    "       \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])  \n",
    "        \n",
    "       \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action])\n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Épisode {episode + 1}, Récompense totale : {total_rewards}\")\n",
    "\n",
    "    \n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01)  \n",
    "\n",
    "\n",
    "print(\"\\nQ-table finale après l'entraînement :\")\n",
    "print(Q_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836fc454-1562-4d7a-9f06-767943dea5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'espace d'actions: Discrete(4)\n",
      "l'espace d'observations: Discrete(16)\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 4, Reward: 0.0, Done: False, action: 3\n",
      "State: 8, Reward: 0.0, Done: False, action: 1\n",
      "State: 9, Reward: 0.0, Done: False, action: 1\n",
      "State: 13, Reward: 0.0, Done: False, action: 2\n",
      "State: 12, Reward: 0.0, Done: True, action: 0\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 2\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 5, Reward: 0.0, Done: True, action: 2\n",
      "State: 0, Reward: 0.0, Done: False, action: 0\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 1, Reward: 0.0, Done: False, action: 0\n",
      "State: 5, Reward: 0.0, Done: True, action: 0\n",
      "State: 4, Reward: 0.0, Done: False, action: 2\n",
      "State: 8, Reward: 0.0, Done: False, action: 1\n",
      "State: 12, Reward: 0.0, Done: True, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 2\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 5, Reward: 0.0, Done: True, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 0\n",
      "State: 0, Reward: 0.0, Done: False, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 0\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 2, Reward: 0.0, Done: False, action: 3\n",
      "State: 6, Reward: 0.0, Done: False, action: 2\n",
      "State: 10, Reward: 0.0, Done: False, action: 1\n",
      "State: 9, Reward: 0.0, Done: False, action: 1\n",
      "State: 10, Reward: 0.0, Done: False, action: 2\n",
      "State: 6, Reward: 0.0, Done: False, action: 2\n",
      "State: 7, Reward: 0.0, Done: True, action: 2\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 5, Reward: 0.0, Done: True, action: 1\n",
      "State: 4, Reward: 0.0, Done: False, action: 0\n",
      "State: 5, Reward: 0.0, Done: True, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 2, Reward: 0.0, Done: False, action: 1\n",
      "State: 1, Reward: 0.0, Done: False, action: 0\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 5, Reward: 0.0, Done: True, action: 0\n",
      "State: 0, Reward: 0.0, Done: False, action: 0\n",
      "State: 4, Reward: 0.0, Done: False, action: 0\n",
      "State: 8, Reward: 0.0, Done: False, action: 2\n",
      "State: 8, Reward: 0.0, Done: False, action: 0\n",
      "State: 8, Reward: 0.0, Done: False, action: 0\n",
      "State: 8, Reward: 0.0, Done: False, action: 1\n",
      "State: 4, Reward: 0.0, Done: False, action: 0\n",
      "State: 5, Reward: 0.0, Done: True, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 1, Reward: 0.0, Done: False, action: 1\n",
      "State: 5, Reward: 0.0, Done: True, action: 1\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 8, Reward: 0.0, Done: False, action: 2\n",
      "State: 4, Reward: 0.0, Done: False, action: 2\n",
      "State: 4, Reward: 0.0, Done: False, action: 0\n",
      "State: 4, Reward: 0.0, Done: False, action: 3\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 2, Reward: 0.0, Done: False, action: 2\n",
      "State: 2, Reward: 0.0, Done: False, action: 3\n",
      "State: 2, Reward: 0.0, Done: False, action: 2\n",
      "State: 6, Reward: 0.0, Done: False, action: 1\n",
      "State: 7, Reward: 0.0, Done: True, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 2, Reward: 0.0, Done: False, action: 1\n",
      "State: 6, Reward: 0.0, Done: False, action: 0\n",
      "State: 10, Reward: 0.0, Done: False, action: 2\n",
      "State: 6, Reward: 0.0, Done: False, action: 3\n",
      "State: 2, Reward: 0.0, Done: False, action: 2\n",
      "State: 3, Reward: 0.0, Done: False, action: 1\n",
      "State: 2, Reward: 0.0, Done: False, action: 1\n",
      "State: 3, Reward: 0.0, Done: False, action: 1\n",
      "State: 2, Reward: 0.0, Done: False, action: 0\n",
      "State: 3, Reward: 0.0, Done: False, action: 3\n",
      "State: 3, Reward: 0.0, Done: False, action: 3\n",
      "State: 7, Reward: 0.0, Done: True, action: 2\n",
      "State: 0, Reward: 0.0, Done: False, action: 2\n",
      "State: 0, Reward: 0.0, Done: False, action: 0\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 0, Reward: 0.0, Done: False, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 2\n",
      "State: 4, Reward: 0.0, Done: False, action: 1\n",
      "State: 5, Reward: 0.0, Done: True, action: 1\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 5, Reward: 0.0, Done: True, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 2\n",
      "State: 1, Reward: 0.0, Done: False, action: 2\n",
      "State: 0, Reward: 0.0, Done: False, action: 1\n",
      "State: 0, Reward: 0.0, Done: False, action: 0\n",
      "State: 0, Reward: 0.0, Done: False, action: 3\n",
      "State: 4, Reward: 0.0, Done: False, action: 2\n",
      "Q_table initialisée:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "État 0: [0. 0. 0. 0.]\n",
      "État 1: [0. 0. 0. 0.]\n",
      "État 2: [0. 0. 0. 0.]\n",
      "État 3: [0. 0. 0. 0.]\n",
      "État 4: [0. 0. 0. 0.]\n",
      "État 5: [0. 0. 0. 0.]\n",
      "État 6: [0. 0. 0. 0.]\n",
      "État 7: [0. 0. 0. 0.]\n",
      "État 8: [0. 0. 0. 0.]\n",
      "État 9: [0. 0. 0. 0.]\n",
      "État 10: [0. 0. 0. 0.]\n",
      "État 11: [0. 0. 0. 0.]\n",
      "État 12: [0. 0. 0. 0.]\n",
      "État 13: [0. 0. 0. 0.]\n",
      "État 14: [0. 0. 0. 0.]\n",
      "État 15: [0. 0. 0. 0.]\n",
      "\n",
      "--- Q-table après 1000 épisodes ---\n",
      "[[4.45606161e-01 3.72585765e-01 3.32408107e-01 3.23823415e-01]\n",
      " [3.38337909e-01 4.40535437e-03 1.71148664e-02 8.53661501e-03]\n",
      " [2.21585192e-02 1.92654008e-01 7.99113913e-05 0.00000000e+00]\n",
      " [6.85177302e-05 6.84445504e-02 0.00000000e+00 5.68106882e-03]\n",
      " [4.58309633e-01 3.67653923e-01 3.06612304e-01 2.87080322e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.45724416e-01 0.00000000e+00 2.90585488e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.38234797e-01 3.66039092e-01 2.38970291e-01 4.75042313e-01]\n",
      " [3.83146375e-01 5.01844454e-01 2.99618592e-01 2.90830088e-01]\n",
      " [4.64730893e-01 2.01094500e-01 1.61778099e-01 1.73974223e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.53156608e-01 1.34379461e-01 6.17134533e-01 2.19611575e-01]\n",
      " [3.67432185e-01 3.61636550e-01 7.60977811e-01 3.44562508e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# exercice1\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(f\"l'espace d'actions: {env.action_space}\")\n",
    "print(f\"l'espace d'observations: {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    print(f\"State: {obs}, Reward: {reward}, Done: {done}, action: {action}\")\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "# exercice2\n",
    "Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(\"Q_table initialisée:\")\n",
    "print(Q_table)\n",
    "\n",
    "for state in range(env.observation_space.n):\n",
    "    print(f\"État {state}: {Q_table[state]}\")\n",
    "\n",
    "# exercice3\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 5000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action])\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        print(f\"\\n--- Q-table après {episode + 1} épisodes ---\")\n",
    "        print(Q_table)\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "\n",
    "print(\"\\nQ-table finale après l'entraînement :\")\n",
    "print(Q_table)\n",
    "\n",
    "success_count = 0\n",
    "num_test_episodes = 100\n",
    "print(\"Début du test en mode exploitation...\")\n",
    "\n",
    "for ep in range(num_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q_table[state])\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if reward == 1.0:\n",
    "                success_count += 1\n",
    "\n",
    "print(f\"\\nTaux de réussite après l'entraînement : {success_count} / {test_episodes} ({success_count/test_episodes*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6e57f-b733-4487-ab09-dfae311f1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# ===> Création de l'environnement <===\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()\n",
    "print(f\"L'espace d'actions: {env.action_space}\")\n",
    "print(f\"L'espace d'observations: {env.observation_space}\")\n",
    "\n",
    "# ===> Exécution de 100 actions aléatoires et affichage des résultats <===\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    print(f\"State: {obs}, Reward: {reward}, Done: {done}, Action: {action}\")\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "# ===> Initialisation de la Q-table <===\n",
    "Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(\"\\nQ_table initialisée:\")\n",
    "print(Q_table)\n",
    "\n",
    "# ===> Affichage des valeurs initiales <===\n",
    "for state in range(env.observation_space.n):\n",
    "    print(f\"État {state}: {Q_table[state]}\")\n",
    "\n",
    "# ===> Paramètres du Q-learning <===\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 5000\n",
    "\n",
    "# ===> Entraînement de l'agent <===\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # Réinitialiser l'environnement\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        # Politique ε-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])\n",
    "\n",
    "        # Exécuter l'action et récupérer les résultats\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Mettre à jour la Q-table\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action])\n",
    "\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    # Afficher la Q-table tous les 500 épisodes\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        print(f\"\\n--- Q-table après {episode + 1} épisodes ---\")\n",
    "        print(Q_table)\n",
    "\n",
    "    # Réduction progressive de epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "\n",
    "# ===> Affichage de la Q-table finale <===\n",
    "print(\"\\nQ-table finale après l'entraînement :\")\n",
    "print(Q_table)\n",
    "\n",
    "# ===> Évaluation de l'agent après entraînement <===\n",
    "success_count = 0\n",
    "test_episodes = 100\n",
    "\n",
    "print(\"\\nDébut du test en mode exploitation...\\n\")\n",
    "\n",
    "for ep in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q_table[state])  # Toujours choisir la meilleure action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "        if done and reward == 1.0:\n",
    "            success_count += 1  # L'agent a atteint l'objectif\n",
    "\n",
    "print(f\"\\nTaux de réussite après l'entraînement : {success_count} / {test_episodes} ({success_count/test_episodes*100:.2f}%)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
